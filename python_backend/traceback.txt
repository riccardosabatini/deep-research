============================= test session starts ==============================
platform darwin -- Python 3.11.6, pytest-9.0.1, pluggy-1.6.0 -- /Users/riccardo/anaconda3/bin/python
cachedir: .pytest_cache
rootdir: /Users/riccardo/Develop/playground/deep-research/python_backend
plugins: mock-3.15.1, anyio-4.9.0, asyncio-1.3.0, langsmith-0.3.45
asyncio: mode=Mode.STRICT, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
collecting ... collected 2 items

tests/test_resume.py::test_resume_functionality FAILED                   [ 50%]
tests/test_resume.py::test_new_session_with_thread_id FAILED             [100%]

=================================== FAILURES ===================================
__________________________ test_resume_functionality ___________________________

self = <langgraph.graph.state.CompiledStateGraph object at 0x11a5a1710>
input = {'final_report': '', 'query': 'Test Query', 'report_plan': '', 'search_results': [], ...}
config = {'callbacks': None, 'configurable': {'thread_id': 'test_resume_thread'}, 'metadata': ChainMap({'thread_id': 'test_resume_thread'}), 'recursion_limit': 25, ...}

    async def astream(
        self,
        input: dict[str, Any] | Any,
        config: RunnableConfig | None = None,
        *,
        stream_mode: StreamMode | list[StreamMode] | None = None,
        output_keys: str | Sequence[str] | None = None,
        interrupt_before: All | Sequence[str] | None = None,
        interrupt_after: All | Sequence[str] | None = None,
        checkpoint_during: bool | None = None,
        debug: bool | None = None,
        subgraphs: bool = False,
    ) -> AsyncIterator[dict[str, Any] | Any]:
        """Asynchronously stream graph steps for a single input.
    
        Args:
            input: The input to the graph.
            config: The configuration to use for the run.
            stream_mode: The mode to stream output, defaults to `self.stream_mode`.
                Options are:
    
                - `"values"`: Emit all values in the state after each step, including interrupts.
                    When used with functional API, values are emitted once at the end of the workflow.
                - `"updates"`: Emit only the node or task names and updates returned by the nodes or tasks after each step.
                    If multiple updates are made in the same step (e.g. multiple nodes are run) then those updates are emitted separately.
                - `"custom"`: Emit custom data from inside nodes or tasks using `StreamWriter`.
                - `"messages"`: Emit LLM messages token-by-token together with metadata for any LLM invocations inside nodes or tasks.
                    Will be emitted as 2-tuples `(LLM token, metadata)`.
                - `"debug"`: Emit debug events with as much information as possible for each step.
    
                You can pass a list as the `stream_mode` parameter to stream multiple modes at once.
                The streamed outputs will be tuples of `(mode, data)`.
    
                See [LangGraph streaming guide](https://langchain-ai.github.io/langgraph/how-tos/streaming/) for more details.
            output_keys: The keys to stream, defaults to all non-context channels.
            interrupt_before: Nodes to interrupt before, defaults to all nodes in the graph.
            interrupt_after: Nodes to interrupt after, defaults to all nodes in the graph.
            checkpoint_during: Whether to checkpoint intermediate steps, defaults to True. If False, only the final checkpoint is saved.
            debug: Whether to print debug information during execution, defaults to False.
            subgraphs: Whether to stream events from inside subgraphs, defaults to False.
                If True, the events will be emitted as tuples `(namespace, data)`,
                or `(namespace, mode, data)` if `stream_mode` is a list,
                where `namespace` is a tuple with the path to the node where a subgraph is invoked,
                e.g. `("parent_node:<task_id>", "child_node:<task_id>")`.
    
                See [LangGraph streaming guide](https://langchain-ai.github.io/langgraph/how-tos/streaming/) for more details.
    
        Yields:
            The output of each step in the graph. The output shape depends on the stream_mode.
        """
    
        stream = AsyncQueue()
        aioloop = asyncio.get_running_loop()
        stream_put = cast(
            Callable[[StreamChunk], None],
            partial(aioloop.call_soon_threadsafe, stream.put_nowait),
        )
    
        def output() -> Iterator:
            while True:
                try:
                    ns, mode, payload = stream.get_nowait()
                except asyncio.QueueEmpty:
                    break
                if subgraphs and isinstance(stream_mode, list):
                    yield (ns, mode, payload)
                elif isinstance(stream_mode, list):
                    yield (mode, payload)
                elif subgraphs:
                    yield (ns, payload)
                else:
                    yield payload
    
        config = ensure_config(self.config, config)
        callback_manager = get_async_callback_manager_for_config(config)
        run_manager = await callback_manager.on_chain_start(
            None,
            input,
            name=config.get("run_name", self.get_name()),
            run_id=config.get("run_id"),
        )
        # if running from astream_log() run each proc with streaming
        do_stream = (
            next(
                (
                    True
                    for h in run_manager.handlers
                    if isinstance(h, _StreamingCallbackHandler)
                    and not isinstance(h, StreamMessagesHandler)
                ),
                False,
            )
            if _StreamingCallbackHandler is not None
            else False
        )
        try:
            # assign defaults
            (
                debug,
                stream_modes,
                output_keys,
                interrupt_before_,
                interrupt_after_,
                checkpointer,
                store,
                cache,
            ) = self._defaults(
                config,
                stream_mode=stream_mode,
                output_keys=output_keys,
                interrupt_before=interrupt_before,
                interrupt_after=interrupt_after,
                debug=debug,
            )
            # set up subgraph checkpointing
            if self.checkpointer is True:
                ns = cast(str, config[CONF][CONFIG_KEY_CHECKPOINT_NS])
                config[CONF][CONFIG_KEY_CHECKPOINT_NS] = recast_checkpoint_ns(ns)
            # set up messages stream mode
            if "messages" in stream_modes:
                run_manager.inheritable_handlers.append(
                    StreamMessagesHandler(stream_put)
                )
            # set up custom stream mode
            if "custom" in stream_modes:
                config[CONF][CONFIG_KEY_STREAM_WRITER] = (
                    lambda c: aioloop.call_soon_threadsafe(
                        stream.put_nowait, ((), "custom", c)
                    )
                )
            # set checkpointing mode for subgraphs
            if checkpoint_during is not None:
                config[CONF][CONFIG_KEY_CHECKPOINT_DURING] = checkpoint_during
            async with AsyncPregelLoop(
                input,
                input_model=self.input_model,
                stream=StreamProtocol(stream.put_nowait, stream_modes),
                config=config,
                store=store,
                cache=cache,
                checkpointer=checkpointer,
                nodes=self.nodes,
                specs=self.channels,
                output_keys=output_keys,
                stream_keys=self.stream_channels_asis,
                interrupt_before=interrupt_before_,
                interrupt_after=interrupt_after_,
                manager=run_manager,
                debug=debug,
                checkpoint_during=checkpoint_during
                if checkpoint_during is not None
                else config[CONF].get(CONFIG_KEY_CHECKPOINT_DURING, True),
                trigger_to_nodes=self.trigger_to_nodes,
                migrate_checkpoint=self._migrate_checkpoint,
                retry_policy=self.retry_policy,
                cache_policy=self.cache_policy,
            ) as loop:
                # create runner
                runner = PregelRunner(
                    submit=config[CONF].get(
                        CONFIG_KEY_RUNNER_SUBMIT, weakref.WeakMethod(loop.submit)
                    ),
                    put_writes=weakref.WeakMethod(loop.put_writes),
                    use_astream=do_stream,
                    node_finished=config[CONF].get(CONFIG_KEY_NODE_FINISHED),
                )
                # enable subgraph streaming
                if subgraphs:
                    loop.config[CONF][CONFIG_KEY_STREAM] = StreamProtocol(
                        stream_put, stream_modes
                    )
                # enable concurrent streaming
                if (
                    self.stream_eager
                    or subgraphs
                    or "messages" in stream_modes
                    or "custom" in stream_modes
                ):
    
                    def get_waiter() -> asyncio.Task[None]:
                        return aioloop.create_task(stream.wait())
    
                else:
                    get_waiter = None  # type: ignore[assignment]
                # Similarly to Bulk Synchronous Parallel / Pregel model
                # computation proceeds in steps, while there are channel updates
                # channel updates from step N are only visible in step N+1
                # channels are guaranteed to be immutable for the duration of the step,
                # with channel updates applied only at the transition between steps
>               while loop.tick(input_keys=self.input_channels):
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

../../../../anaconda3/lib/python3.11/site-packages/langgraph/pregel/__init__.py:2652: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <langgraph.pregel.loop.AsyncPregelLoop object at 0x11a5a6a90>

    def tick(
        self,
        *,
        input_keys: Union[str, Sequence[str]],
    ) -> bool:
        """Execute a single iteration of the Pregel loop.
    
        Args:
            input_keys: The key(s) to read input from.
    
        Returns:
            True if more iterations are needed.
        """
        if self.status != "pending":
            raise RuntimeError("Cannot tick when status is no longer 'pending'")
    
        updated_channels: set[str] | None = None
    
        if self.input not in (INPUT_DONE, INPUT_RESUMING, INPUT_SHOULD_VALIDATE):
            updated_channels = self._first(input_keys=input_keys)
        elif self.to_interrupt:
            # if we need to interrupt, do so
            self.status = "interrupt_before"
            raise GraphInterrupt()
        elif all(task.writes for task in self.tasks.values()):
            # finish superstep
            writes = [w for t in self.tasks.values() for w in t.writes]
            # debug flag
            if self.debug:
                print_step_writes(
                    self.step,
                    writes,
                    (
                        [self.stream_keys]
                        if isinstance(self.stream_keys, str)
                        else self.stream_keys
                    ),
                )
            # all tasks have finished
            mv_writes, updated_channels = apply_writes(
                self.checkpoint,
                self.channels,
                self.tasks.values(),
                self.checkpointer_get_next_version,
                self.trigger_to_nodes,
            )
            # apply writes to managed values
            for key, values in mv_writes.items():
                self._update_mv(key, values)
            # validate input if requested
            if self.input is INPUT_SHOULD_VALIDATE:
                self.input = INPUT_DONE
                # validate
                cast(type[BaseModel], self.input_model)(
                    **read_channels(self.channels, self.stream_keys)
                )
            # produce values output
            if not updated_channels.isdisjoint(
                (self.output_keys,)
                if isinstance(self.output_keys, str)
                else self.output_keys
            ):
                self._emit(
                    "values", map_output_values, self.output_keys, writes, self.channels
                )
            # clear pending writes
            self.checkpoint_pending_writes.clear()
            # "not skip_done_tasks" only applies to first tick after resuming
            self.skip_done_tasks = True
            # save checkpoint
            self._put_checkpoint(
                {
                    "source": "loop",
                    "writes": single(
                        map_output_updates(
                            self.output_keys,
                            [(t, t.writes) for t in self.tasks.values()],
                        )
                    ),
                }
            )
            # after execution, check if we should interrupt
            if self.interrupt_after and should_interrupt(
                self.checkpoint, self.interrupt_after, self.tasks.values()
            ):
                self.status = "interrupt_after"
                raise GraphInterrupt()
    
            # unset resuming flag
            self.config[CONF].pop(CONFIG_KEY_RESUMING, None)
        else:
            return False
    
        # check if iteration limit is reached
        if self.step > self.stop:
            self.status = "out_of_steps"
            return False
    
        # prepare next tasks
        self.tasks = prepare_next_tasks(
            self.checkpoint,
            self.checkpoint_pending_writes,
            self.nodes,
            self.channels,
            self.managed,
            self.config,
            self.step,
            for_execution=True,
            manager=self.manager,
            store=self.store,
            checkpointer=self.checkpointer,
            trigger_to_nodes=self.trigger_to_nodes,
            updated_channels=updated_channels,
            retry_policy=self.retry_policy,
            cache_policy=self.cache_policy,
        )
        self.to_interrupt = []
    
        # produce debug output
        if self._checkpointer_put_after_previous is not None:
            self._emit(
                "debug",
                map_debug_checkpoint,
                self.step - 1,  # printing checkpoint for previous step
                self.checkpoint_config,
                self.channels,
                self.stream_keys,
                self.checkpoint_metadata,
                self.checkpoint,
                self.tasks.values(),
                self.checkpoint_pending_writes,
                self.prev_checkpoint_config,
                self.output_keys,
            )
    
        # if no more tasks, we're done
        if not self.tasks:
            self.status = "done"
            return False
    
        # check if we should delegate (used by subgraphs in distributed mode)
        if self.config[CONF].get(CONFIG_KEY_DELEGATE):
            assert self.input is INPUT_RESUMING
            raise GraphDelegate(
                {
                    "config": patch_configurable(
                        self.config, {CONFIG_KEY_DELEGATE: False}
                    ),
                    "input": None,
                }
            )
    
        # if there are pending writes from a previous loop, apply them
        if self.skip_done_tasks and self.checkpoint_pending_writes:
            self._match_writes(self.tasks)
    
        # if all tasks have finished, re-tick
        if all(task.writes for task in self.tasks.values()):
            return self.tick(input_keys=input_keys)
    
        # before execution, check if we should interrupt
        if self.interrupt_before and should_interrupt(
            self.checkpoint, self.interrupt_before, self.tasks.values()
        ):
            self.status = "interrupt_before"
>           raise GraphInterrupt()
E           langgraph.errors.GraphInterrupt: ()

../../../../anaconda3/lib/python3.11/site-packages/langgraph/pregel/loop.py:619: GraphInterrupt

During handling of the above exception, another exception occurred:

tmp_path = PosixPath('/private/var/folders/tc/gk5smsts51z33jz8zprzgq_40000gn/T/pytest-of-riccardo/pytest-9/test_resume_functionality0')
mock_llm = <MagicMock spec='ChatOpenAI' id='4736723728'>
mock_search_tools = <MagicMock id='4736857360'>
mocker = <pytest_mock.plugin.MockerFixture object at 0x11a489050>
capsys = <_pytest.capture.CaptureFixture object at 0x11a56f7d0>

    @pytest.mark.asyncio
    async def test_resume_functionality(tmp_path, mock_llm, mock_search_tools, mocker, capsys):
        # Setup temp DB
        db_path = tmp_path / "test_resume.db"
        db_uri = str(db_path)
    
        # Mock Config to use temp DB
        mock_config = MagicMock(spec=Config)
        mock_config.db_provider = "sqlite"
        mock_config.db_uri = db_uri
        mock_config.log_level = "INFO"
        mocker.patch("src.configuration.Config.from_env", return_value=mock_config)
    
        # Mock dependencies
        mocker.patch("src.nodes.get_llm", return_value=mock_llm)
        mocker.patch("src.nodes.search_tools", mock_search_tools)
        mocker.patch("src.nodes.get_search_result", return_value=None)
        mocker.patch("src.nodes.save_search_result", new_callable=AsyncMock)
    
        # Setup mock responses for LLM
        mock_plan = AIMessage(content="Mocked Plan")
        mock_learnings = AIMessage(content="Mocked Learnings")
        mock_report = AIMessage(content="Mocked Report")
    
        # We need enough side effects for two runs.
        # Run 1: Plan -> Generate -> Search -> Review (Pause)
        # Run 2: Resume -> Review (Input) -> Write Report
    
        # Calls in Run 1:
        # 1. plan_research (ainvoke)
        # 2. generate_queries (structured output)
        # 3. perform_search (ainvoke - learnings)
    
        # Calls in Run 2:
        # 4. write_report (ainvoke)
    
        mock_llm.ainvoke.side_effect = [mock_plan, mock_learnings, mock_report, mock_report]
        mock_llm.invoke.side_effect = [mock_plan, mock_learnings, mock_report, mock_report]
    
        mock_queries = DeepResearchQueryList(queries=[
            DeepResearchSearchTask(query="q1", research_goal="g1")
        ])
        mock_llm.with_structured_output.return_value.ainvoke.return_value = mock_queries
    
        # Mock input to simulate user pressing Enter (empty string) to finish review
        mocker.patch("builtins.input", return_value="")
    
        # Mock console to capture output? Or use capsys?
        # main.py uses a global console object. We might need to patch it or just check capsys.
        # Let's patch console.print to verify messages.
        mock_console = MagicMock()
        mocker.patch("main.console", mock_console)
    
        thread_id = "test_resume_thread"
    
        # --- Run 1: Start New Session ---
        # We want to interrupt it. run_research runs until completion or interruption.
        # But run_research in main.py has a "Review Loop" that waits for input.
        # If we provide input="", it proceeds to write report and finishes.
        # To test "Resume", we need to simulate a crash or stop BEFORE the review loop finishes?
        # Or we can run it once fully, then run it again?
        # If we run it once fully, it finishes. Resuming a finished workflow might just return the result.
    
        # We want to verify "Resuming existing research session..." message.
        # If we run it once, it will populate the state.
        # If we run it again with same thread_id, it should say "Resuming...".
    
        print("--- Starting Run 1 ---")
>       report1 = await run_research("Test Query", thread_id=thread_id)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/test_resume.py:78: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
main.py:105: in run_research
    await _run_graph(checkpointer, query, thread_id)
main.py:140: in _run_graph
    async for event in app.astream(initial_state, config=run_config):
../../../../anaconda3/lib/python3.11/site-packages/langgraph/pregel/__init__.py:2596: in astream
    async with AsyncPregelLoop(
../../../../anaconda3/lib/python3.11/site-packages/langgraph/pregel/loop.py:1393: in __aexit__
    return await exit_task
           ^^^^^^^^^^^^^^^
../../../../anaconda3/lib/python3.11/contextlib.py:733: in __aexit__
    raise exc_details[1]
../../../../anaconda3/lib/python3.11/contextlib.py:716: in __aexit__
    cb_suppress = await cb(*exc_details)
                  ^^^^^^^^^^^^^^^^^^^^^^
../../../../anaconda3/lib/python3.11/site-packages/langgraph/pregel/executor.py:209: in __aexit__
    raise exc
../../../../anaconda3/lib/python3.11/site-packages/langgraph/pregel/loop.py:1260: in _checkpointer_put_after_previous
    await prev
../../../../anaconda3/lib/python3.11/site-packages/langgraph/pregel/loop.py:1262: in _checkpointer_put_after_previous
    await cast(BaseCheckpointSaver, self.checkpointer).aput(
../../../../anaconda3/lib/python3.11/site-packages/langgraph/checkpoint/sqlite/aio.py:503: in aput
    serialized_metadata = json.dumps(
../../../../anaconda3/lib/python3.11/json/__init__.py:238: in dumps
    **kw).encode(obj)
          ^^^^^^^^^^^
../../../../anaconda3/lib/python3.11/json/encoder.py:200: in encode
    chunks = self.iterencode(o, _one_shot=True)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../../../../anaconda3/lib/python3.11/json/encoder.py:258: in iterencode
    return _iterencode(o, 0)
           ^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <json.encoder.JSONEncoder object at 0x11a6116d0>
o = DeepResearchSearchTask(query='q1', research_goal='g1')

    def default(self, o):
        """Implement this method in a subclass such that it returns
        a serializable object for ``o``, or calls the base implementation
        (to raise a ``TypeError``).
    
        For example, to support arbitrary iterators, you could
        implement default like this::
    
            def default(self, o):
                try:
                    iterable = iter(o)
                except TypeError:
                    pass
                else:
                    return list(iterable)
                # Let the base class default method raise the TypeError
                return JSONEncoder.default(self, o)
    
        """
>       raise TypeError(f'Object of type {o.__class__.__name__} '
                        f'is not JSON serializable')
E       TypeError: Object of type DeepResearchSearchTask is not JSON serializable

../../../../anaconda3/lib/python3.11/json/encoder.py:180: TypeError
----------------------------- Captured stdout call -----------------------------
--- Starting Run 1 ---
╭─────────────────────────────── Plan Research ────────────────────────────────╮
│ Planning Research for: Test Query                                            │
╰──────────────────────────────────────────────────────────────────────────────╯
╭────────────────────────────── Generate Queries ──────────────────────────────╮
│ Generating Queries...                                                        │
╰──────────────────────────────────────────────────────────────────────────────╯
Executing Search: q1
_______________________ test_new_session_with_thread_id ________________________

self = <langgraph.graph.state.CompiledStateGraph object at 0x11af11810>
input = {'final_report': '', 'query': 'Test Query', 'report_plan': '', 'search_results': [], ...}
config = {'callbacks': None, 'configurable': {'thread_id': 'new_thread_id'}, 'metadata': ChainMap({'thread_id': 'new_thread_id'}), 'recursion_limit': 25, ...}

    async def astream(
        self,
        input: dict[str, Any] | Any,
        config: RunnableConfig | None = None,
        *,
        stream_mode: StreamMode | list[StreamMode] | None = None,
        output_keys: str | Sequence[str] | None = None,
        interrupt_before: All | Sequence[str] | None = None,
        interrupt_after: All | Sequence[str] | None = None,
        checkpoint_during: bool | None = None,
        debug: bool | None = None,
        subgraphs: bool = False,
    ) -> AsyncIterator[dict[str, Any] | Any]:
        """Asynchronously stream graph steps for a single input.
    
        Args:
            input: The input to the graph.
            config: The configuration to use for the run.
            stream_mode: The mode to stream output, defaults to `self.stream_mode`.
                Options are:
    
                - `"values"`: Emit all values in the state after each step, including interrupts.
                    When used with functional API, values are emitted once at the end of the workflow.
                - `"updates"`: Emit only the node or task names and updates returned by the nodes or tasks after each step.
                    If multiple updates are made in the same step (e.g. multiple nodes are run) then those updates are emitted separately.
                - `"custom"`: Emit custom data from inside nodes or tasks using `StreamWriter`.
                - `"messages"`: Emit LLM messages token-by-token together with metadata for any LLM invocations inside nodes or tasks.
                    Will be emitted as 2-tuples `(LLM token, metadata)`.
                - `"debug"`: Emit debug events with as much information as possible for each step.
    
                You can pass a list as the `stream_mode` parameter to stream multiple modes at once.
                The streamed outputs will be tuples of `(mode, data)`.
    
                See [LangGraph streaming guide](https://langchain-ai.github.io/langgraph/how-tos/streaming/) for more details.
            output_keys: The keys to stream, defaults to all non-context channels.
            interrupt_before: Nodes to interrupt before, defaults to all nodes in the graph.
            interrupt_after: Nodes to interrupt after, defaults to all nodes in the graph.
            checkpoint_during: Whether to checkpoint intermediate steps, defaults to True. If False, only the final checkpoint is saved.
            debug: Whether to print debug information during execution, defaults to False.
            subgraphs: Whether to stream events from inside subgraphs, defaults to False.
                If True, the events will be emitted as tuples `(namespace, data)`,
                or `(namespace, mode, data)` if `stream_mode` is a list,
                where `namespace` is a tuple with the path to the node where a subgraph is invoked,
                e.g. `("parent_node:<task_id>", "child_node:<task_id>")`.
    
                See [LangGraph streaming guide](https://langchain-ai.github.io/langgraph/how-tos/streaming/) for more details.
    
        Yields:
            The output of each step in the graph. The output shape depends on the stream_mode.
        """
    
        stream = AsyncQueue()
        aioloop = asyncio.get_running_loop()
        stream_put = cast(
            Callable[[StreamChunk], None],
            partial(aioloop.call_soon_threadsafe, stream.put_nowait),
        )
    
        def output() -> Iterator:
            while True:
                try:
                    ns, mode, payload = stream.get_nowait()
                except asyncio.QueueEmpty:
                    break
                if subgraphs and isinstance(stream_mode, list):
                    yield (ns, mode, payload)
                elif isinstance(stream_mode, list):
                    yield (mode, payload)
                elif subgraphs:
                    yield (ns, payload)
                else:
                    yield payload
    
        config = ensure_config(self.config, config)
        callback_manager = get_async_callback_manager_for_config(config)
        run_manager = await callback_manager.on_chain_start(
            None,
            input,
            name=config.get("run_name", self.get_name()),
            run_id=config.get("run_id"),
        )
        # if running from astream_log() run each proc with streaming
        do_stream = (
            next(
                (
                    True
                    for h in run_manager.handlers
                    if isinstance(h, _StreamingCallbackHandler)
                    and not isinstance(h, StreamMessagesHandler)
                ),
                False,
            )
            if _StreamingCallbackHandler is not None
            else False
        )
        try:
            # assign defaults
            (
                debug,
                stream_modes,
                output_keys,
                interrupt_before_,
                interrupt_after_,
                checkpointer,
                store,
                cache,
            ) = self._defaults(
                config,
                stream_mode=stream_mode,
                output_keys=output_keys,
                interrupt_before=interrupt_before,
                interrupt_after=interrupt_after,
                debug=debug,
            )
            # set up subgraph checkpointing
            if self.checkpointer is True:
                ns = cast(str, config[CONF][CONFIG_KEY_CHECKPOINT_NS])
                config[CONF][CONFIG_KEY_CHECKPOINT_NS] = recast_checkpoint_ns(ns)
            # set up messages stream mode
            if "messages" in stream_modes:
                run_manager.inheritable_handlers.append(
                    StreamMessagesHandler(stream_put)
                )
            # set up custom stream mode
            if "custom" in stream_modes:
                config[CONF][CONFIG_KEY_STREAM_WRITER] = (
                    lambda c: aioloop.call_soon_threadsafe(
                        stream.put_nowait, ((), "custom", c)
                    )
                )
            # set checkpointing mode for subgraphs
            if checkpoint_during is not None:
                config[CONF][CONFIG_KEY_CHECKPOINT_DURING] = checkpoint_during
            async with AsyncPregelLoop(
                input,
                input_model=self.input_model,
                stream=StreamProtocol(stream.put_nowait, stream_modes),
                config=config,
                store=store,
                cache=cache,
                checkpointer=checkpointer,
                nodes=self.nodes,
                specs=self.channels,
                output_keys=output_keys,
                stream_keys=self.stream_channels_asis,
                interrupt_before=interrupt_before_,
                interrupt_after=interrupt_after_,
                manager=run_manager,
                debug=debug,
                checkpoint_during=checkpoint_during
                if checkpoint_during is not None
                else config[CONF].get(CONFIG_KEY_CHECKPOINT_DURING, True),
                trigger_to_nodes=self.trigger_to_nodes,
                migrate_checkpoint=self._migrate_checkpoint,
                retry_policy=self.retry_policy,
                cache_policy=self.cache_policy,
            ) as loop:
                # create runner
                runner = PregelRunner(
                    submit=config[CONF].get(
                        CONFIG_KEY_RUNNER_SUBMIT, weakref.WeakMethod(loop.submit)
                    ),
                    put_writes=weakref.WeakMethod(loop.put_writes),
                    use_astream=do_stream,
                    node_finished=config[CONF].get(CONFIG_KEY_NODE_FINISHED),
                )
                # enable subgraph streaming
                if subgraphs:
                    loop.config[CONF][CONFIG_KEY_STREAM] = StreamProtocol(
                        stream_put, stream_modes
                    )
                # enable concurrent streaming
                if (
                    self.stream_eager
                    or subgraphs
                    or "messages" in stream_modes
                    or "custom" in stream_modes
                ):
    
                    def get_waiter() -> asyncio.Task[None]:
                        return aioloop.create_task(stream.wait())
    
                else:
                    get_waiter = None  # type: ignore[assignment]
                # Similarly to Bulk Synchronous Parallel / Pregel model
                # computation proceeds in steps, while there are channel updates
                # channel updates from step N are only visible in step N+1
                # channels are guaranteed to be immutable for the duration of the step,
                # with channel updates applied only at the transition between steps
>               while loop.tick(input_keys=self.input_channels):
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

../../../../anaconda3/lib/python3.11/site-packages/langgraph/pregel/__init__.py:2652: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <langgraph.pregel.loop.AsyncPregelLoop object at 0x11af45690>

    def tick(
        self,
        *,
        input_keys: Union[str, Sequence[str]],
    ) -> bool:
        """Execute a single iteration of the Pregel loop.
    
        Args:
            input_keys: The key(s) to read input from.
    
        Returns:
            True if more iterations are needed.
        """
        if self.status != "pending":
            raise RuntimeError("Cannot tick when status is no longer 'pending'")
    
        updated_channels: set[str] | None = None
    
        if self.input not in (INPUT_DONE, INPUT_RESUMING, INPUT_SHOULD_VALIDATE):
            updated_channels = self._first(input_keys=input_keys)
        elif self.to_interrupt:
            # if we need to interrupt, do so
            self.status = "interrupt_before"
            raise GraphInterrupt()
        elif all(task.writes for task in self.tasks.values()):
            # finish superstep
            writes = [w for t in self.tasks.values() for w in t.writes]
            # debug flag
            if self.debug:
                print_step_writes(
                    self.step,
                    writes,
                    (
                        [self.stream_keys]
                        if isinstance(self.stream_keys, str)
                        else self.stream_keys
                    ),
                )
            # all tasks have finished
            mv_writes, updated_channels = apply_writes(
                self.checkpoint,
                self.channels,
                self.tasks.values(),
                self.checkpointer_get_next_version,
                self.trigger_to_nodes,
            )
            # apply writes to managed values
            for key, values in mv_writes.items():
                self._update_mv(key, values)
            # validate input if requested
            if self.input is INPUT_SHOULD_VALIDATE:
                self.input = INPUT_DONE
                # validate
                cast(type[BaseModel], self.input_model)(
                    **read_channels(self.channels, self.stream_keys)
                )
            # produce values output
            if not updated_channels.isdisjoint(
                (self.output_keys,)
                if isinstance(self.output_keys, str)
                else self.output_keys
            ):
                self._emit(
                    "values", map_output_values, self.output_keys, writes, self.channels
                )
            # clear pending writes
            self.checkpoint_pending_writes.clear()
            # "not skip_done_tasks" only applies to first tick after resuming
            self.skip_done_tasks = True
            # save checkpoint
            self._put_checkpoint(
                {
                    "source": "loop",
                    "writes": single(
                        map_output_updates(
                            self.output_keys,
                            [(t, t.writes) for t in self.tasks.values()],
                        )
                    ),
                }
            )
            # after execution, check if we should interrupt
            if self.interrupt_after and should_interrupt(
                self.checkpoint, self.interrupt_after, self.tasks.values()
            ):
                self.status = "interrupt_after"
                raise GraphInterrupt()
    
            # unset resuming flag
            self.config[CONF].pop(CONFIG_KEY_RESUMING, None)
        else:
            return False
    
        # check if iteration limit is reached
        if self.step > self.stop:
            self.status = "out_of_steps"
            return False
    
        # prepare next tasks
        self.tasks = prepare_next_tasks(
            self.checkpoint,
            self.checkpoint_pending_writes,
            self.nodes,
            self.channels,
            self.managed,
            self.config,
            self.step,
            for_execution=True,
            manager=self.manager,
            store=self.store,
            checkpointer=self.checkpointer,
            trigger_to_nodes=self.trigger_to_nodes,
            updated_channels=updated_channels,
            retry_policy=self.retry_policy,
            cache_policy=self.cache_policy,
        )
        self.to_interrupt = []
    
        # produce debug output
        if self._checkpointer_put_after_previous is not None:
            self._emit(
                "debug",
                map_debug_checkpoint,
                self.step - 1,  # printing checkpoint for previous step
                self.checkpoint_config,
                self.channels,
                self.stream_keys,
                self.checkpoint_metadata,
                self.checkpoint,
                self.tasks.values(),
                self.checkpoint_pending_writes,
                self.prev_checkpoint_config,
                self.output_keys,
            )
    
        # if no more tasks, we're done
        if not self.tasks:
            self.status = "done"
            return False
    
        # check if we should delegate (used by subgraphs in distributed mode)
        if self.config[CONF].get(CONFIG_KEY_DELEGATE):
            assert self.input is INPUT_RESUMING
            raise GraphDelegate(
                {
                    "config": patch_configurable(
                        self.config, {CONFIG_KEY_DELEGATE: False}
                    ),
                    "input": None,
                }
            )
    
        # if there are pending writes from a previous loop, apply them
        if self.skip_done_tasks and self.checkpoint_pending_writes:
            self._match_writes(self.tasks)
    
        # if all tasks have finished, re-tick
        if all(task.writes for task in self.tasks.values()):
            return self.tick(input_keys=input_keys)
    
        # before execution, check if we should interrupt
        if self.interrupt_before and should_interrupt(
            self.checkpoint, self.interrupt_before, self.tasks.values()
        ):
            self.status = "interrupt_before"
>           raise GraphInterrupt()
E           langgraph.errors.GraphInterrupt: ()

../../../../anaconda3/lib/python3.11/site-packages/langgraph/pregel/loop.py:619: GraphInterrupt

During handling of the above exception, another exception occurred:

tmp_path = PosixPath('/private/var/folders/tc/gk5smsts51z33jz8zprzgq_40000gn/T/pytest-of-riccardo/pytest-9/test_new_session_with_thread_i0')
mock_llm = <MagicMock spec='ChatOpenAI' id='4736931984'>
mock_search_tools = <MagicMock id='4746334032'>
mocker = <pytest_mock.plugin.MockerFixture object at 0x11a8bde50>

    @pytest.mark.asyncio
    async def test_new_session_with_thread_id(tmp_path, mock_llm, mock_search_tools, mocker):
        # Setup temp DB
        db_path = tmp_path / "test_new.db"
        db_uri = str(db_path)
    
        mock_config = MagicMock(spec=Config)
        mock_config.db_provider = "sqlite"
        mock_config.db_uri = db_uri
        mock_config.log_level = "INFO"
        mocker.patch("src.configuration.Config.from_env", return_value=mock_config)
    
        mocker.patch("src.nodes.get_llm", return_value=mock_llm)
        mocker.patch("src.nodes.search_tools", mock_search_tools)
        mocker.patch("src.nodes.get_search_result", return_value=None)
        mocker.patch("src.nodes.save_search_result", new_callable=AsyncMock)
        mocker.patch("builtins.input", return_value="")
    
        mock_console = MagicMock()
        mocker.patch("main.console", mock_console)
    
        mock_plan = AIMessage(content="Mocked Plan")
        mock_learnings = AIMessage(content="Mocked Learnings")
        mock_report = AIMessage(content="Mocked Report")
    
        mock_llm.ainvoke.side_effect = [mock_plan, mock_learnings, mock_report]
        mock_llm.invoke.side_effect = [mock_plan, mock_learnings, mock_report]
    
        mock_queries = DeepResearchQueryList(queries=[
            DeepResearchSearchTask(query="q1", research_goal="g1")
        ])
        mock_llm.with_structured_output.return_value.ainvoke.return_value = mock_queries
    
        # Call with a NEW thread_id
        thread_id = "new_thread_id"
>       await run_research("Test Query", thread_id=thread_id)

tests/test_resume.py:166: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
main.py:105: in run_research
    await _run_graph(checkpointer, query, thread_id)
main.py:140: in _run_graph
    async for event in app.astream(initial_state, config=run_config):
../../../../anaconda3/lib/python3.11/site-packages/langgraph/pregel/__init__.py:2596: in astream
    async with AsyncPregelLoop(
../../../../anaconda3/lib/python3.11/site-packages/langgraph/pregel/loop.py:1393: in __aexit__
    return await exit_task
           ^^^^^^^^^^^^^^^
../../../../anaconda3/lib/python3.11/contextlib.py:733: in __aexit__
    raise exc_details[1]
../../../../anaconda3/lib/python3.11/contextlib.py:716: in __aexit__
    cb_suppress = await cb(*exc_details)
                  ^^^^^^^^^^^^^^^^^^^^^^
../../../../anaconda3/lib/python3.11/site-packages/langgraph/pregel/executor.py:209: in __aexit__
    raise exc
../../../../anaconda3/lib/python3.11/site-packages/langgraph/pregel/loop.py:1260: in _checkpointer_put_after_previous
    await prev
../../../../anaconda3/lib/python3.11/site-packages/langgraph/pregel/loop.py:1262: in _checkpointer_put_after_previous
    await cast(BaseCheckpointSaver, self.checkpointer).aput(
../../../../anaconda3/lib/python3.11/site-packages/langgraph/checkpoint/sqlite/aio.py:503: in aput
    serialized_metadata = json.dumps(
../../../../anaconda3/lib/python3.11/json/__init__.py:238: in dumps
    **kw).encode(obj)
          ^^^^^^^^^^^
../../../../anaconda3/lib/python3.11/json/encoder.py:200: in encode
    chunks = self.iterencode(o, _one_shot=True)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
../../../../anaconda3/lib/python3.11/json/encoder.py:258: in iterencode
    return _iterencode(o, 0)
           ^^^^^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <json.encoder.JSONEncoder object at 0x11a812a90>
o = DeepResearchSearchTask(query='q1', research_goal='g1')

    def default(self, o):
        """Implement this method in a subclass such that it returns
        a serializable object for ``o``, or calls the base implementation
        (to raise a ``TypeError``).
    
        For example, to support arbitrary iterators, you could
        implement default like this::
    
            def default(self, o):
                try:
                    iterable = iter(o)
                except TypeError:
                    pass
                else:
                    return list(iterable)
                # Let the base class default method raise the TypeError
                return JSONEncoder.default(self, o)
    
        """
>       raise TypeError(f'Object of type {o.__class__.__name__} '
                        f'is not JSON serializable')
E       TypeError: Object of type DeepResearchSearchTask is not JSON serializable

../../../../anaconda3/lib/python3.11/json/encoder.py:180: TypeError
----------------------------- Captured stdout call -----------------------------
╭─────────────────────────────── Plan Research ────────────────────────────────╮
│ Planning Research for: Test Query                                            │
╰──────────────────────────────────────────────────────────────────────────────╯
╭────────────────────────────── Generate Queries ──────────────────────────────╮
│ Generating Queries...                                                        │
╰──────────────────────────────────────────────────────────────────────────────╯
Executing Search: q1
=============================== warnings summary ===============================
../../../../anaconda3/lib/python3.11/site-packages/langgraph/checkpoint/postgres/__init__.py:26
  /Users/riccardo/anaconda3/lib/python3.11/site-packages/langgraph/checkpoint/postgres/__init__.py:26: DeprecationWarning: You're using incompatible versions of langgraph and checkpoint-postgres. Please upgrade langgraph to avoid unexpected behavior.
    from langgraph.checkpoint.postgres.base import BasePostgresSaver

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ============================
FAILED tests/test_resume.py::test_resume_functionality - TypeError: Object of type DeepResearchSearchTask is not JSON serializable
FAILED tests/test_resume.py::test_new_session_with_thread_id - TypeError: Object of type DeepResearchSearchTask is not JSON serializable
========================= 2 failed, 1 warning in 0.42s =========================
